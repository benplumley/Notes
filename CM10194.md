#CM10194 - Computer Systems Architecture I
##Dr Fabio Nemetz <small>Labs: Daniela, Christina, Saeid, Gideon</small>

###29/09/2014
**Analogue Computing**  
Analogue variables are *continuously variable*, and have precisely exact values. However, when their values are read, either by a human or the analogue computer system, they lose their precision and thus their accuracy.

**Digital Computing**  
Digital computers use *discrete* data values to represent variables. This means they are imprecise (because a temperature recorded as 23.1C might actually be 23.098659...C) but their values can be read with absolute precision.

###30/09/2014
**First Generation of Digital Computers**  
The first generation of computers used vacuum tubes (valves) and were around in the 1940s. Examples include Bletchley Park's *Colossus* and von Neumann's *ENIAC*.

**Second Generation**  
The second generation used a relatively new invention, the *transistor*. This allowed a small current to control the flow of a larger one. Compared with valves, transistors were smaller, more durable with a longer lifespan, used less power and produced less heat. They were also cheaper than their glass counterparts, meaning computers could be smaller, cheaper to make and easier to maintain. Among the first commercial computers to use them was IBM's *608* from 1957.

**Third Generation**  
The third generation improved on the transistor by putting many of them onto the same piece of silicon. This was called the *integrated circuit*, or chip. The first chips, in 1961, had only a few tens or hundreds of transistors per chip.

**Fourth Generation**  
The fourth generation was the evolution of the chip - called *Large Scale Integration*, or LSI, an integrated circuit in the 1970s might have had tens of thousands of transistors on it.

**Fifth Generation**  
This is the current generation of computers. It began in the mid 1980s and is known as *Very Large Scale Integration*, or VLSI. A VLSI chip can have several billion transistors.

**Von Neumann Architecture**  
This is what almost all commercial laptop and desktop computers use today. Named after its creator, John von Neumann, this architecture is based around the idea that a computer inherently contains five components:  
- The *Arithmetic and Logic Unit*, or ALU  
- The Control Unit, which sends instructions (operators) and data (operands) to the ALU  
- The data store, which contains both data and instructions. In this context, it refers to the RAM  
- The input devices (mouse, keyboard)  
- The output devices (screen, printer)  
Crucial to this architecture is that the same data store is used for the data as is used for the instructions. This follows from the principle that instructions can be represented as data, and is known as the *stored program concept*. The CPU is made up of the ALU and its Control Unit.

There are drawbacks to the Von Neumann architecture. Because the CPU can process data faster than it can be written to the data store, a bottleneck can form between the CPU and memory. This is exacerbated by the processing of small files, where the ratio of data to be processed to 'overhead' (metadata, address locations etc) is low. This has become more of a problem as CPU speed has increased at a greater rate than the speed at which memory can be written to. It can be alleviated through the use of caches, which are small fast memory locations near the processor.

**Harvard Architecture**  
The main difference between the Von Neumann architecture and the Harvard architecture is the data store. The Harvard architecture uses two separate data stores; one for data and the other for programs. Embedded devices, including members of the Arduino line, use this architecture.
